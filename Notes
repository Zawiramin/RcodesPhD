#----------------------------------------------------------------------------------------#
#' To build a decision tree model, we will use the rpart function from the rpart package.
#' In the function, there are several elements:
#' 1- formula = science_perf ~ . defines the dependent variable (i.e., science_perf) and the 
#'    predictors (and ~ is the separator). Because we use science_perf ~ ., we use all variables 
#'    in the dataset (except for science_perf) as our predictors. We could also write the same 
#'    formula as science_perf ~ math + reading + ESCS + ... + WEALTH by specifying each 
#'    variable individually.
#' 2- data = pisaM_train defines the dataset we are using for the analysis.
#' 3- method = "class" defines what type of decision tree we are building. method = "class" defines 
#'    a classification tree and method = "anova" defines a regression tree.
#' 4- control is a list of control (i.e., tuning) elements for the decision tree algorithm. 
#'    minsplit defines the minimum number of observations that must exist in a node (default = 20); 
#'    cp is the complexity parameter to prune the subtrees that don’t improve the model fit 
#'    (default = 0.01, if cp = 0, then no pruning); 
#'    xval is the number of cross-validations (default = 10, if xval = 0, then no cross validation).
#' 5- parms is a list of optional parameters for the splitting function. 
#'    anova splitting (i.e., regression trees) has no parameters. 
#'    For class splitting (i.e., classification tree), the most important option is 
#'    the split index – which is either "gini" for the Gini index or 
#'    "information" for the Entropy index. 
#'    Splitting based on information can be slightly slower compared to the Gini index 
#'    (see the vignette for more information).
#' https://okanbulut.github.io/bigdata/supervised-machine-learning-part-i.html#decision-trees-in-r

#' #-------------------------------------------------------------------------------------# 
#' THE PLAN:
#' Building [Model 1] Tree Using Gini Index
#' [Model 1]- SCIENCE LEARNING with Gini Index with xval = 10 
#' [Model 1a]- SCIENCE LEARNING with Gini Index with 70-30
#' [Model 1b]- SCIENCE LEARNING with Gini Index with 70-30 & XVAL 10
#' 
#' Building [Model 2] Tree Using Entropy Index (Information Gain)
#' [Model 2]- SCIENCE LEARNING with Information Gain with xval = 10 
#' [Model 2a]- SCIENCE LEARNING with Information Gain with 70-30
#' [Model 2b]- SCIENCE LEARNING with Information Gain with 70-30 & XVAL 10


#' #-------------------------------------------------------------------------------------# 
#' Building [Model 1] Tree Using Gini Index with xval = 10 [ALL Variables]
#' 
#' First model use pisaM_nm (pisa malaysia new model - NA was romoved) using xval
scLM1 <- rpart(formula = science_perf ~ .,
                    data = scLearnNew,
                    method = "class", 
                    parms = list(split = "gini"),
                    control = rpart.control(minsplit = 20, cp = 0,xval = 10))

#' In default, When rpart grows a tree it performs 10-fold cross validation on the data.
#' Since we not specified any xval, there'd be cross validation used in the model.
#' Cross-validations (e.g., K-fold approach) are highly useful when we do not 
#' have a test or validation dataset, or our dataset is to small to split into training and test data.
#' Use printcp() to see the cross validation results.

printcp(scLM1)

#' The rel error of each iteration of the tree is the fraction of mislabeled elements in the iteration 
#' relative to the fraction of mislabeled elements in the root. In this example, 50% of training cases 
#' are fraudulent. 
#' The first splitting criteria is “Is the claimant very active?”, which separates the data into a set 
#' of three cases, all of which are fraudulent and a set of seven cases of which two are fraudulent. Labeling 
#' the cases at this point would produce an error rate of 20% which is 40% of the root node error rate 
#' (i.e. it’s 60% better). 
#' The cross validation error rates and standard deviations are displayed in the columns xerror and xstd 
#' respectively.
#' As a rule of thumb, it’s best to prune a decision tree using the cp of smallest tree that is within one 
#' standard deviation of the tree with the smallest xerror. In this example, the best xerror is 0.4 with 
#' standard deviation 0.25298. So, we want the smallest tree with xerror less than 0.65298. 
#' This is the tree with cp = 0.2, so we’ll want to prune our tree with a cp slightly greater than than 0.2.

#' In the output, CP refers to the complexity parameter, nsplit is the number of splits in 
#' the decision tree based on the complexity parameter, and rel error is the relative error 
#' (i.e.,  1−Rsquared) of the solution. 

#' This is the error for predictions of the data that 
#' were used to estimate the model. The section of Variables actually used in tree construction
#' shows which variables have been used in the final model. What happened to the other variables?
#' newFit
#' add pruning features to keep only the most important splits
#' https://www.edureka.co/blog/implementation-of-decision-tree/
#' 
cp.prune <- scLM1$cptable[which.min(scLM1$cptable[,"xerror"]),"CP"]
#' This function returns the optimal cp value associated with the minimum error.
cp.prune
#' 
#' From the above mentioned list of cp values, we can select the one having the least cross-validated error and use it to prune the tree.
#' The value of cp should be least, so that the cross-validated error rate is minimum.
#' 
plotcp(scLM1)
#' Plotcp() provides a graphical representation to the cross validated error summary. The cp values 
#' are plotted against the geometric mean to depict the deviation until the minimum value is reached.
#' From the above mentioned list of cp values, we can select the one having the least 
#' cross-validated error (xerror) and use it to prune the tree.

#' We can use rpart.rules to see more clreare of the model
#' read from the bottom (http://www.milbo.org/doc/prp.pdf)

#' then using the obtained CP with the 10fold xval
scLM1new <- rpart(formula = science_perf ~ .,
                       data = scLearnNew,
                       method = "class", 
                       parms = list(split = "gini"),
                       control = rpart.control(minsplit = 20, cp = cp.prune, xval = 0))
#' read from the bottom (http://www.milbo.org/doc/prp.pdf)
rpart.rules(scLM1new, cover = TRUE)
#' Both of these show the importance of the variables for our estimated decision tree model. 
#' The larger the values are, the more crucial they are for the model.
#' To see what happened to other variables, we can use summary() and varImp() 
summary(scLM1new)
varImp(scLM1new)


#rpart.plot(pisaM_fit1new,
#           main = "Decision Tree Model 1 (Gini)",
#           box.palette = "GnBu",cex.main=1.2,fallen.leaves = TRUE,
#           nn=FALSE,clip.right.labs = FALSE,branch=.3,type = 2,
#           yesno=2,extra = 8)

#'
#' Then, check the classification accuracy of the estimated decision tree with the test data. 
#' Otherwise, it is hard to justify whether or not the estimated decision tree would work 
#' accurately for prediction. 
#' 
#' Below we estimate the predicted classes (either high or low) from 
#' the test data by applying the estimated model.
#' 
#' First we obtain model predictions using predict() and then turn the results into a data frame 
#' called pisaM.pred.
scL.pred <- predict(scLM1new, scLearnNew) %>%
  as.data.frame()
head(scL.pred)
#'
#' This dataset shows each observation’s (i.e., students from the test data) probability of falling 
#' into either high or low categories based on the decision rules that we estimated. We will turn 
#' these probabilities into binary classifications, depending on whether or not they are >=50%. 
#' Then, we will compare these estimates with the actual classes in the test data 
#' (i.e., test_dat$science_perf) in order to create a confusion matrix.
#' 
scL.pred <- mutate(scL.pred,
                     science_perf = as.factor(ifelse(High >= 0.5, "High", "Low"))) %>% 
  select(science_perf)

#' Confusion Matrix 1
confusionMatrix(scL.pred$science_perf, scLearnNew$science_perf,mode = "everything")
#' 


#' #-------------------------------------------------------------------------------------# 
#' Building [Model 1a] using Gini Index with xval = 10
#' modelNew = extracted from Model 1 
#' 
modelNew <- science_perf ~ changeStructure+classDebate+
  classNeeds+computer+delayWorking+demonstrateIdea+
  designOwnExp+doingPractExp+
  ESCS+explScIdeas+
  helpGetJob+
  howToImprove+howToReachGoals+
  importantFor+learning+
  myAreaToImprove+myStrength+noise+
  notListen+quiteDown+
  software+techExpl+
  worthIt+worthWhile

pisaM_fit1a <- rpart(formula = modelNew,
                     data = pisaM_train,
                     method = "class", 
                     parms = list(split = "gini"),
                     control = rpart.control(minsplit = 20, cp = 0, xval = 10))
#' When rpart grows a tree it performs 10-fold cross validation on the data.
#' Use printcp() to see the cross validation results.
printcp(pisaM_fit1a)
#' newFit
#' add pruning features to keep only the most important splits
cp.prune1a <- pisaM_fit1a$cptable[which.min(pisaM_fit1a$cptable[,"xerror"]),"CP"]
cp.prune1a
#' 

#' We can use rpart.rules to see more clreare of the model
#' read from the bottom
pisaM_fit1aNew <- rpart(formula = modelNew,
                        data = pisaM_train,
                        method = "class", 
                        parms = list(split = "information"),
                        control = rpart.control(minsplit = 20, cp = cp.prune1a, xval = 0))

rpart.rules(pisaM_fit1aNew, cover = TRUE)
#' To see what happened to other variables, we can use summary() and varImp() 
summary(pisaM_fit1aNew)
varImp(pisaM_fit1aNew)

#rpart.plot(pisaM_fit1aNew,
#           main = "Decision Tree Model 1 (Gini)",
#           box.palette = "GnBu",cex.main=1.2,fallen.leaves = TRUE,
#           nn=FALSE,clip.right.labs = FALSE,branch=.3,type = 2,
#           yesno=2,extra = 8)

#' Then, check the classification accuracy of the estimated decision tree with the test data. 
#' Otherwise, it is hard to justify whether or not the estimated decision tree would work 
#' accurately for prediction. Below we estimate the predicted classes (either high or low) from 
#' the test data by applying the estimated model.
#' First we obtain model predictions using predict() and then turn the results into a data frame 
#' called pisaM.pred.
pisaM.pred1a <- predict(pisaM_fit1aNew, pisaM_test) %>%
  as.data.frame()
head(pisaM.pred1a)
#'
#' This dataset shows each observation’s (i.e., students from the test data) probability of falling 
#' into either high or low categories based on the decision rules that we estimated. We will turn 
#' these probabilities into binary classifications, depending on whether or not they are >=50%. 
#' Then, we will compare these estimates with the actual classes in the test data 
#' (i.e., test_dat$science_perf) in order to create a confusion matrix.
#' 
pisaM.pred1a <- mutate(pisaM.pred1a,
                       science_perf = as.factor(ifelse(High >= 0.5, "High", "Low"))) %>% 
  select(science_perf)

#' Confusion Matrix 2
confusionMatrix(pisaM.pred1a$science_perf, pisaM_test$science_perf)

#'
#' #-------------------------------------------------------------------------------------# 
#' Building [Model 2] Tree Using Gini Index with splitting data
#' 
pisaM_fit2 <- rpart(formula = science_perf ~.,                 
                    data = pisaM_train,
                    method = "class", 
                    parms = list(split = "gini"))
printcp(pisaM_fit2)
#' newFit
#' add pruning features to keep only the most important splits
cp.prune2 <- pisaM_fit2$cptable[which.min(pisaM_fit2$cptable[,"xerror"]),"CP"]
cp.prune2
pisaM_fit2New <- rpart(formula = science_perf ~ .,
                       data = pisaM_train,
                       method = "class", 
                       parms = list(split = "gini"),
                       control = rpart.control(minsplit = 20, cp = cp.prune2,xval = 0))

#' To see what happened to other variables, we can use summary() and varImp() 
summary(pisaM_fit2New)
varImp(pisaM_fit2New)
#' We can use rpart.rules to see more clreare of the model
#' read from the bottom
#' 
#' 
rpart.rules(pisaM_fit2.prune, cover = TRUE)
rpart.plot(pisaM_fit2.prune,
           main = "Decision Tree Model 2 (Gini)",
           box.palette = "GnBu",cex.main=1.2,fallen.leaves = TRUE,
           nn=TRUE,clip.right.labs = FALSE,branch=.3,type = 2,
           yesno=2)
#' Then, check the classification accuracy of the estimated decision tree with the test data. 
#' Otherwise, it is hard to justify whether or not the estimated decision tree would work 
#' accurately for prediction. Below we estimate the predicted classes (either high or low) from 
#' the test data by applying the estimated model.
#' First we obtain model predictions using predict() and then turn the results into a data frame 
#' called pisaM.pred.
pisaM.pred2 <- predict(pisaM_fit2New, pisaM_test) %>%
  as.data.frame()
head(pisaM.pred2)
#'
#' This dataset shows each observation’s (i.e., students from the test data) probability of falling 
#' into either high or low categories based on the decision rules that we estimated. We will turn 
#' these probabilities into binary classifications, depending on whether or not they are >=50%. 
#' Then, we will compare these estimates with the actual classes in the test data 
#' (i.e., test_dat$science_perf) in order to create a confusion matrix.
#' 
pisaM.pred2 <- mutate(pisaM.pred2,
                      science_perf = as.factor(ifelse(High >= 0.5, "High", "Low"))) %>% 
  select(science_perf)

#' Confusion Matrix 1
confusionMatrix(pisaM.pred2$science_perf, pisaM_test$science_perf)
#' #-------------------------------------------------------------------------------------# 
#' Building [Model 2a]Tree Using Entropy Index (Information Gain) with splitting data

pisaM_fit2a <- rpart(formula = science_perf ~ .,                     
                     data = pisaM_train,
                     method = "class", 
                     parms = list(split = "information",xval = 0))
printcp(pisaM_fit2a)
#' newFit
#' add pruning features to keep only the most important splits
cp.prune2a <- pisaM_fit2a$cptable[which.min(pisaM_fit2a$cptable[,"xerror"]),"CP"]
cp.prune2a
#' To see what happened to other variables, we can use summary() and varImp() 
summary(pisaM_fit2a)
varImp(pisaM_fit2a)
#' We can use rpart.rules to see more clreare of the model
#' read from the bottom
rpart.rules(pisaM_fit2a, cover = TRUE)
rpart.plot(pisaM_fit2a,
           main = "Decision Tree Model 2a (Information Gain)",
           box.palette = "GnBu",cex.main=1.2,fallen.leaves = TRUE,
           nn=TRUE,clip.right.labs = FALSE,branch=.3,type = 2,
           yesno=2)
#' Then, check the classification accuracy of the estimated decision tree with the test data. 
#' Otherwise, it is hard to justify whether or not the estimated decision tree would work 
#' accurately for prediction. Below we estimate the predicted classes (either high or low) from 
#' the test data by applying the estimated model.
#' First we obtain model predictions using predict() and then turn the results into a data frame 
#' called pisaM.pred.
pisaM.pred2a <- predict(pisaM_fit2a, pisaM_test) %>%
  as.data.frame()
head(pisaM.pred2a)
#'
#' This dataset shows each observation’s (i.e., students from the test data) probability of falling 
#' into either high or low categories based on the decision rules that we estimated. We will turn 
#' these probabilities into binary classifications, depending on whether or not they are >=50%. 
#' Then, we will compare these estimates with the actual classes in the test data 
#' (i.e., test_dat$science_perf) in order to create a confusion matrix.
#' 
pisaM.pred2a <- mutate(pisaM.pred2a,
                       science_perf = as.factor(ifelse(High >= 0.5, "High", "Low"))) %>% 
  select(science_perf)

#' Confusion Matrix 1
confusionMatrix(pisaM.pred2a$science_perf, pisaM_test$science_perf)
